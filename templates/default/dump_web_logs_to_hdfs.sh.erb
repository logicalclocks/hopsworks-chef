#!/usr/bin/env bash

LOGS_DIR=<%= @weblogs_dir %>
HADOOP_HOME=<%= node['hops']['base_dir'] %>
HDFS_WEBLOGS_DIR=<%= @remote_weblogs_dir %>

CHECKPOINT=$LOGS_DIR/.checkpoint
FILES_TO_DUMP=$LOGS_DIR/.files_to_dump
REMOTE_FILE_PERMISSIONS=0750
# This will match only files in the format of server_access_log.2017-12-19-13_22.txt
# Year could be 5 digits :)
REGEX=".*server_access_log\.[0-9]{4,5}\-[0-9]{1,2}\-[0-9]{1,2}\-[0-9]{2}\_[0-9]{2}\.txt"

function printer {
    echo "<Web logs dumper> $1"
}

if [ -e $CHECKPOINT ]
then
    printer "Dumping log files since `cat $CHECKPOINT`"
    find $LOGS_DIR -type f -regextype posix-extended -regex $REGEX -readable -newer $CHECKPOINT  > $FILES_TO_DUMP
    cp $CHECKPOINT $CHECKPOINT.bak
else
    printer "Checkpoint does not exist, dumping all"
    find $LOGS_DIR -type f -regextype posix-extended -regex $REGEX -readable > $FILES_TO_DUMP
fi

echo `date` > $CHECKPOINT

function failed_exit {
    rm -f $FILES_TO_DUMP
    mv $CHECKPOINT.bak $CHECKPOINT
    printer $1
    exit $2
}

while IFS='' read -r log_file || [[ -n "$log_file" ]];
do
    # Create directory in HDFS
    filename=$(basename "$log_file")
    date=$(echo $filename | awk -F '.' {'print $2'})
    year=$(echo $date | awk -F '-' {'print $1'})
    month=$(echo $date | awk -F '-' {'print $2'})
    REMOTE_DIR=$HDFS_WEBLOGS_DIR/$year/$month

    $HADOOP_HOME/bin/hdfs dfs -test -d $REMOTE_DIR
    if [ $? -ne 0 ]
    then
	printer "Creating remote directory $REMOTE_DIR"
	$HADOOP_HOME/bin/hdfs dfs -mkdir -p $REMOTE_DIR
	if [ $? -ne 0 ]
	then
	    failed_exit "Failed to create directory $REMOTE_DIR. Exiting..." 1
	fi
    fi
    
    full_remote_file=$REMOTE_DIR/$filename
    printer "Copying file $log_file"
    # Check if the file already exists. If it does then Glassfish has appended to that
    # file during rotation. Delete the previous file and put the new.
    $HADOOP_HOME/bin/hdfs dfs -test -e $full_remote_file
    if [ $? -eq 0 ]
    then
	printer "File $log_file already exist, removing it and putting the new"
	$HADOOP_HOME/bin/hdfs dfs -rm -f $full_remote_file
    fi
    
    $HADOOP_HOME/bin/hdfs dfs -copyFromLocal $log_file $REMOTE_DIR
    if [ $? -ne 0 ]
    then
        failed_exit "Error while copying $log_file. Exiting..." 2
    fi

    # Change permissions to 0750
    $HADOOP_HOME/bin/hdfs dfs -chmod $REMOTE_FILE_PERMISSIONS $full_remote_file
done < $FILES_TO_DUMP

printer "Backup finished successfully. Deleting temporary files."
rm -f $FILES_TO_DUMP
rm -f $CHECKPOINT.bak
